#### 需求

[题目地址](https://github.com/loong/go-concurrency-exercises/tree/main/0-limit-crawler)

将main.go中的爬虫程序修改为限制每秒最多爬一个页面

```go
//////////////////////////////////////////////////////////////////////
//
// Your task is to change the code to limit the crawler to at most one
// page per second, while maintaining concurrency (in other words,
// Crawl() must be called concurrently)
//
// @hint: you can achieve this by adding 3 lines
//

package main

import (
	"fmt"
	"sync"
)

// Crawl uses `fetcher` from the `mockfetcher.go` file to imitate a
// real crawler. It crawls until the maximum depth has reached.
func Crawl(url string, depth int, wg *sync.WaitGroup) {
	defer wg.Done()

	if depth <= 0 {
		return
	}
	body, urls, err := fetcher.Fetch(url)
	if err != nil {
		fmt.Println(err)
		return
	}

	fmt.Printf("found: %s %q\n", url, body)

	wg.Add(len(urls))
	for _, u := range urls {
		// Do not remove the `go` keyword, as Crawl() must be
		// called concurrently
		go Crawl(u, depth-1, wg)
	}
}

func main() {
	var wg sync.WaitGroup

	wg.Add(1)
	Crawl("http://golang.org/", 4, &wg)
	wg.Wait()
}
```



#### 结果

```go
// 限制爬虫过度请求的程序
package main

import (
	"fmt"
	"sync"
	"time"
)

// 第 1 行：定义全局的 rateLimiter 通道
var rateLimiter = make(chan struct{}, 1)

func Crawl(url string, depth int, wg *sync.WaitGroup) {
	defer wg.Done()

	if depth <= 0 {
		return
	}
	// 第 2 行：在请求前获取令牌
	// rateLimiter通道中每秒会有一个信号,如果没有就等着有,如果有就直接运行下面的代码
	<-rateLimiter
	body, urls, err := fetcher.Fetch(url)
	if err != nil {
		fmt.Println(err)
		return
	}

	fmt.Printf("found: %s %q\n", url, body)

	wg.Add(len(urls))
	for _, u := range urls {
		// Do not remove the `go` keyword, as Crawl() must be
		// called concurrently
		go Crawl(u, depth-1, wg)
	}
}

func main() {
	var wg sync.WaitGroup
	// 第 3 行：启动 Goroutine 定期放入令牌
	go func() {
		// 创建一个定时器,每秒生成一个时间
		ticker := time.NewTicker(1 * time.Second)
		defer ticker.Stop()
		// 无线循环, 每秒执行一次
		for range ticker.C {
			// 专门用于通道,哪个case可以执行,就执行哪个case
			select {
			// 如果rateLimiter通道没有信息, 这个case就可以执行,如果有就执行default
			case rateLimiter <- struct{}{}:
			default:
			}
		}
	}()
	wg.Add(1)
	Crawl("http://golang.org/", 4, &wg)
	wg.Wait()
}

```

#### 总结

- 利用了令牌桶的思想, 固定的速率产生令牌
- 这里使用了定时器在固定时间生成一个时间
- 通过select执行往通道中发送信号
- 在具体爬虫的函数中,只有通道中有信号了才能继续执行, 通道中的信号又是固定速率才能产生,这样就能控制速率