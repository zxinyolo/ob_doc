>影响数据系统设计的因素很多，包括参与人员的技能和经验、历史遗留问题、系统路径依赖、交付时限、公司的风险容忍度、监管约束等

#### 可靠性，可伸缩性，可维护性

1. 可靠性：及时出现问题，也能继续正确工作
   1. 硬件故障
      1. 硬件冗余
      2. 云平台
      3. 灵活性，弹性，不是单机可靠性
   2. 软件错误：系统性故障没有速效药
      1. 仔细考虑系统中的假设和交互
      2. 彻底的测试
      3. 进程隔离
      4. 允许进程崩溃并重启
      5. 测量、监控并分析生产环境中的系统行为
      6. 出现问题时报警
   3. 人为错误
      1. 以最小犯错机会的方式设计系统
      2. 最容易犯错的地方解耦
      3. 彻底的测试
      4. 人为错误之后简单快速地恢复
      5. 配置详细和明确的监控
      6. 良好的管理实践
2. 可伸缩性：用来描述系统应对负载增长能力的术语，现在能可靠运行不代表以后也能可靠的运行
   1. 描述负载

      >**背景：**
      >
      >​		用户可以发推（平均每秒4600次，最多时每秒12000次），还可以查看主页时间线
      >
      >
      >
      >**负载：**
      >
      >​		发推其实不难：每秒写入12000条数据，不算高端写入压力
      >
      >​		真正的挑战是展示主页时间线，每秒30万次请求（查询操作）压力巨大
      >
      >​	
      >
      >**实现方式：**
      >
      >​		方式1: 懒惰写入+智能查询
      >
      >​					推文写入时，只插入全局推文表，用户查看主页时，系统再动态查找他   					们关注的所有人，取出这些人人发出的推文，排序合并
      >
      >​					这种方式发推简单，存一次就完了，坏处是每次用户刷新主页，都要做					很多数据库join操作，特别慢，特别耗数据库的资源
      >
      >​		方式2: 提前算好+快速读取
      >
      >​					每个用户都有一个主页时间线缓存（类似收件箱）当某人发推时，系统					把这条推文复制一份发到他的每个粉丝的主页时间线上
      >
      >​					这样用户查看主页非常快，因为推早就塞进去了，坏处是发推特别费   					劲，背后要更新很多人缓存
      >
      >​		最终方式：方式1和方式2混合
      >
      >​					对大部分普通用户常用方式2,发推时直接写入关注者的缓存
      >
      >​					对超大V采用方法1,不进行扇出，而是用户刷新主页时动态去查他们的					推文再合并
      >
      >**总结：**
      >
      >​		推特起初写入轻，读取重，数据库扛不住
      >
      >​		后来改成写入重，读取轻，但名人发推太重
      >
      >​		于是最终使用普通人用方法2,名人用方法1的混合方案，让系统即快又稳定

      1. 可能是每秒像web服务器发出的请求
      2. 数据库库中读写的比例
      3. 聊天室中同时活跃的用户数量
      4. 缓存命中率或其他东西